<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Objects365 Dataset">
    <meta name="author" content="Shuai Shao">

    <title>Detection In the Wild Challenge Workshop 2019</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/modern-business.css" rel="stylesheet">

<style>
.center {
  margin-left: auto;
  margin-right: auto;
  max-width: fit-content;
  padding: 10px;
}

.indented {
	padding-left:250px;
}

.content {
	text-align: left;
}
</style>



  </head>

  <body>

    <header>
<header>
  <!-- Navigation -->
  <script type="text/javascript" src="navigation.js"></script>
</header>

    <!-- Page Content -->
    <div class="container">

      <!-- h1 class="my-4">Welcome to Modern Business</h1 -->
      <h1 class="my-4"></h1>

  <div><h1>Submit Your Method</h1></div> <!-- Apply the center class here -->
      <div class="row" style="margin-top:20px;">
        <div class="col-lg-12">
		<h3>Evaluation</h3>
          <p>
         The benchmark data contains three classes: inlier, outlier, and ignore. Inliers are regions that contain known classes from cityscapes, ignore regions are ambiguous regions that do not contain anomalies nor in the inlier region, and outlier class contains anomalous instances (see Figure 1). We evaluate the predictions only for the outlier class, without focusing on predictions of the inlier regions. The test set contains three dataset with different properties, but shares a common in-distribution dataset. We compute the final score as the average of the individual Average Precision (AP) scores.
	  </p><p>
For validation purposes, we provide a benchmark suite with a labeled FishyScapes (FS LaF) validation set.
	  </p>



	  <p> Please note, we will not approve accounts with email addresses from free email providers, e.g., gmail.com, qq.com, web.de, etc. Only university or company email addresses will get access. See also our terms and conditions.</p>
	  <p>
	  Please read our terms and conditions on our submission policy and our requirements on the email address.</p>

	  <div class="tab-pane active" id="learn_the_details-evaluation">
                    
                        
                        <script>
                            {
                                const content = '\u003Ch1\u003EEvaluation\u003C/h1\u003E\u000A\u003Ch2\u003EData Format and Submission Format\u003C/h2\u003E\u000A\u000A\u003Cp\u003E\u000AThe evaluation process is similar to the CityScapes evaluation framework as outlined in \u003Ca href\u003D\u0022https://github.com/mcordts/cityscapesScripts\u0022\u003ECityscapesScripts\u003C/a\u003E.\u000AA notable adjustment is the minimum object size for evaluation, set to 10 pixels.\u000A\u003C/p\u003E\u000A\u000A\u003Cp\u003E\u000ASubmissions must be packaged as a zip file, submission.zip, containing three specific folders and an optional description file.\u000AThe required structure is detailed below:\u000A\u003C/p\u003E\u000A\u000A\u003Cpre\u003E\u000Asubmission.zip\u000A├── description.txt [Optional]\u000A├── fishyscapes\u000A│   ├── 01_Hanns_Klemm_Str_45_000000_000060_gtCoarse_instanceIds_0_1.png\u000A│   ├── 01_Hanns_Klemm_Str_45_000000_000060_gtCoarse_instanceIds_1_1.png\u000A│   ├── ...\u000A│   ├── 01_Hanns_Klemm_Str_45_000000_000060_gtCoarse_instanceIds_N_1.png\u000A│   ├── 01_Hanns_Klemm_Str_45_000000_000060_gtCoarse_instanceIds_pred.txt\u000A│   ├── ...\u000A│   └── 15_Rechbergstr_Deckenpfronn_000007_000360_gtCoarse_instanceIds_pred.txt\u000A├── roadanomaly\u000A│   ├── airplane0000_InstanceIds_0_1.png\u000A│   ├── airplane0000_InstanceIds_1_1.png\u000A│   ├── ...\u000A│   ├── airplane0000_InstanceIds_N_1.png\u000A│   ├── airplane0000_InstanceIds_pred.txt\u000A│   ├── ...\u000A│   └── zebra0001_InstanceIds_pred.txt\u000A└── roadobstacle\u000A    ├── curvy\u002Dstreet_carton_1_InstanceIds_0_1.png\u000A    ├── curvy\u002Dstreet_carton_1_InstanceIds_1_1.png\u000A    ├── ...\u000A    ├── curvy\u002Dstreet_carton_1_InstanceIds_N_1.png\u000A    ├── curvy\u002Dstreet_carton_1_InstanceIds_pred.txt\u000A    ├── ...\u000A    └── snowstorm2_00_07_27.397_InstanceIds_pred.txt\u000A\u003C/pre\u003E\u000A\u000A\u003Cp\u003E\u000AEach .png file within these folders should be a binary mask, and each .txt file must list the .png files along with a corresponding class and confidence level.\u000AGiven that the evaluation is for a single class (anomaly), the class is always marked as 1.\u000A\u003C/p\u003E\u000A\u000A\u003Cpre\u003E\u000A$ cat 01_Hanns_Klemm_Str_45_000000_000080_gtCoarse_instanceIds_pred.txt\u000A01_Hanns_Klemm_Str_45_000000_000080_gtCoarse_instanceIds_0.png 1 0.1\u000A01_Hanns_Klemm_Str_45_000000_000080_gtCoarse_instanceIds_1.png 1 0.15\u000A...\u000A01_Hanns_Klemm_Str_45_000000_000080_gtCoarse_instanceIds_N.png 1 0.7\u000A\u003C/pre\u003E\u000A\u000A\u003Cp\u003E\u000AThe optional description.txt file allows submitters to provide additional details about their method, including the name of the approach, a link to the paper, and a link to the code repository.\u000AThis information is parsed and displayed in the detailed results section of the submission.\u000AAll entries are optional and we will only parse this file if it\u0027s available.\u000A\u003C/p\u003E\u000A\u000A\u003Cpre\u003E\u000Aname: DummyMethod\u000Apaper_link: https://paper\u000Acode_link: https://code\u000A\u003C/pre\u003E\u000A\u000A\u003Ch2\u003E Evaluation Criteria \u003C/h2\u003E\u000A\u000A\u003Cp\u003E\u000ASubmissions are evaluated based on Average Precision (AP) and AP50, following the CityScapes setup.\u000AParticipants are strongly encouraged to use the verification script available in \u003Ca href\u003D\u0022https://github.com/kumuji/anomaly\u002Dbenchmark\u002Dsuite\u0022\u003Ethe benchmark suite\u003C/a\u003E to ensure compliance with the submission format.\u000AThis is crucial since all submissions count to the overall maximum number of submissions allowed.\u000A\u003C/p\u003E\u000A\u000A\u003Cp\u003E\u003Cstrong\u003EImportant Note:\u003C/strong\u003E\u000AUploading the zip file with your results takes some time and there is (unfortunately) no indication of the status of the upload.\u000AYou will only see that it is being processed when your data has been successfully uploaded.\u000A\u003C/p\u003E\u000A\u000A\u003Ch3\u003EKey Recommendations\u003C/h3\u003E\u000A\u003Cul\u003E\u000A  \u003Cli\u003E\u000A    \u003Cp\u003EUse the provided verification script to avoid unnecessary submission attempts. \u003C/p\u003E\u000A  \u003C/li\u003E\u000A  \u003Cli\u003E\u000A    \u003Cp\u003EPrepare your submission carefully, following the specified file structure and naming conventions. \u003C/p\u003E\u000A  \u003C/li\u003E\u000A  \u003Cli\u003E\u000A    \u003Cp\u003EConsider including a \u003Ccode\u003Edescription.txt\u003C/code\u003E file to offer more information on your method. \u003C/p\u003E\u000A  \u003C/li\u003E\u000A\u003C/ul\u003E\u000A';
                                document.write(DOMPurify.sanitize(
                                    content,
                                    { FORBID_TAGS: ['style'] }
                                ));
                            }
                        </script>
<h5>Data Format and Submission Format</h5>

<p>
The evaluation process is similar to the CityScapes evaluation framework as outlined in <a href="https://github.com/mcordts/cityscapesScripts">CityscapesScripts</a>.
A notable adjustment is the minimum object size for evaluation, set to 10 pixels.
</p>

<p>
Submissions must be packaged as a zip file, submission.zip, containing three specific folders and an optional description file.
The required structure is detailed below:
</p>

<code><pre>submission.zip</pre>
<div class="center"><pre>
├── description.txt [Optional]
├── fishyscapes
│&nbsp;&nbsp; ├── 01_Hanns_Klemm_Str_45_000000_000060_gtCoarse_instanceIds_0_1.png
│&nbsp;&nbsp; ├── 01_Hanns_Klemm_Str_45_000000_000060_gtCoarse_instanceIds_1_1.png
│&nbsp;&nbsp; ├── ...
│&nbsp;&nbsp; ├── 01_Hanns_Klemm_Str_45_000000_000060_gtCoarse_instanceIds_N_1.png
│&nbsp;&nbsp; ├── 01_Hanns_Klemm_Str_45_000000_000060_gtCoarse_instanceIds_pred.txt
│&nbsp;&nbsp; ├── ...
│&nbsp;&nbsp; └── 15_Rechbergstr_Deckenpfronn_000007_000360_gtCoarse_instanceIds_pred.txt
├── roadanomaly
│&nbsp;&nbsp; ├── airplane0000_InstanceIds_0_1.png
│&nbsp;&nbsp; ├── airplane0000_InstanceIds_1_1.png
│&nbsp;&nbsp; ├── ...
│&nbsp;&nbsp; ├── airplane0000_InstanceIds_N_1.png
│&nbsp;&nbsp; ├── airplane0000_InstanceIds_pred.txt
│&nbsp;&nbsp; ├── ...
│&nbsp;&nbsp; └── zebra0001_InstanceIds_pred.txt
└── roadobstacle
    ├── curvy-street_carton_1_InstanceIds_0_1.png
    ├── curvy-street_carton_1_InstanceIds_1_1.png
    ├── ...
    ├── curvy-street_carton_1_InstanceIds_N_1.png
    ├── curvy-street_carton_1_InstanceIds_pred.txt
    ├── ...
    └── snowstorm2_00_07_27.397_InstanceIds_pred.txt
    </pre></div></code>

<p>
Each .png file within these folders should be a binary mask, and each .txt file must list the .png files along with a corresponding class and confidence level.
Given that the evaluation is for a single class (anomaly), the class is always marked as 1.
</p>

<code><pre>$ cat 01_Hanns_Klemm_Str_45_000000_000080_gtCoarse_instanceIds_pred.txt

01_Hanns_Klemm_Str_45_000000_000080_gtCoarse_instanceIds_0.png 1 0.1
01_Hanns_Klemm_Str_45_000000_000080_gtCoarse_instanceIds_1.png 1 0.15
...
01_Hanns_Klemm_Str_45_000000_000080_gtCoarse_instanceIds_N.png 1 0.7
	</pre></code>

<p>
The optional description.txt file allows submitters to provide additional details about their method, including the name of the approach, a link to the paper, and a link to the code repository.
This information is parsed and displayed in the detailed results section of the submission.
All entries are optional and we will only parse this file if it's available.
</p>

<code><pre>name: DummyMethod</pre>
<div class="center">
<pre>paper_link: https://paper
code_link: https://code
</pre></div></code>

<h5> Evaluation Criteria </h5>

<p>
Submissions are evaluated based on Average Precision (AP) and AP50, following the CityScapes setup.
Participants are strongly encouraged to use the verification script available in <a href="https://github.com/kumuji/anomaly-benchmark-suite">the benchmark suite</a> to ensure compliance with the submission format.
This is crucial since all submissions count to the overall maximum number of submissions allowed.
</p>

<p><strong>Important Note:</strong>
Uploading the zip file with your results takes some time and there is (unfortunately) no indication of the status of the upload.
You will only see that it is being processed when your data has been successfully uploaded.
</p>

<h3>Key Recommendations</h3>
<ul>
  <li>
    <p>Use the provided verification script to avoid unnecessary submission attempts. </p>
  </li>
  <li>
    <p>Prepare your submission carefully, following the specified file structure and naming conventions. </p>
  </li>
  <li>
    <p>Consider including a <code>description.txt</code> file to offer more information on your method. </p>
  </li>
</ul>

                        
                    
                </div>

	<h3> Terms and Conditions </h3>
	<p> Only the training set is provided for learning the parameters of the algorithms. The test set should be used only for reporting the final results compared to other approaches - it must not be used in any way to train or tune systems, for example, by evaluating multiple parameters or feature choices and reporting the best results obtained. Thus, we impose an upper limit (currently 10 attempts) on the number of submissions. Model should be run - ideally - only once on the test data, and the results of the test set should not be used to adapt the approach.</p>

	<h3> Submission Policy </h3>

	<p>The evaluation server may not be used for parameter tuning since we rely here on a shared resource that is provided by the Codalab team and its sponsors. We ask each participant to upload the final results of their algorithm/paper submission only once to the server and perform all other experiments on the validation set. If participants would like to report results in their papers for multiple versions of their algorithm (e.g., parameters or features), this must be done on the validation data, and only the best-performing setting of the novel method may be submitted for evaluation to our server.</p>

	<p><strong>Important note</strong>: It is NOT allowed to register multiple times to the server using different email addresses. We are actively monitoring submissions and we will revoke access and delete submissions. When registering with Codalab, we ask all participants to use a unique institutional email address (e.g., .edu) or company email address. We will not approve email addresses from free email services anymore (e.g., gmail.com, hotmail.com, qq.com). If you need to use such an email address, then contact us to approve your account.
        </div>
      </div>
      <!-- /.row -->


    </div>
    <!-- /.container -->

    <!-- Footer -->
    <footer class="py-5 bg-dark">
	  <script type="text/javascript" src="footer.js"></script>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  </body>

</html>
